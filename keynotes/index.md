---
layout: default
title: Keynotes
permalink: /keynotes/
order: 1
---
# Keynotes

![ShapeMI](images/fletcher_headshot2.png)
**Short bio**
Tom Fletcher is an Associate Professor in Electrical and Computer Engineering and also Computer Science at the University of Virginia. Prior to joining UVA, he was faculty at the School of Computing and the Scientific Computing and Imaging Institute at the University of Utah from 2009 - 2018. His main research expertise lies in the areas of statistical modeling of high-dimensional and nonlinear data from images, anatomical shape analysis, and brain connectivity analysis from functional and diffusion MR images. His work in these areas has been recognized with seven best paper awards in the top image analysis and computer vision conferences (MICCAI, IPMI, ISBI, ICCV), workshops (ShapeMI), and journals (MedIA, Signal Processing).

**Talk abstract**
In this talk I will present several ways in which Riemannian geometry plays a role in deep neural networks. The first is in deep generative models, which learn a mapping from a low-dimensional latent space to a high-dimensional data space. Under certain regularity conditions, these models parameterize nonlinear manifolds in the data space. We develop efficient algorithms for computing geodesic curves and distances on such manifolds. Second, I show that classification models also naturally involve manifold geometry. This has implications for adversarial examples, where small perturbations of an input can surprisingly fool an otherwise accurate classifier. Using concepts from information geometry, we show how to find directions that fool a classifier with minimal perturbation. In the final application of manifold geometry, I will present recent work on making deep classifiers more interpretable by relating their geometry to gradients of interpretable features.

![ShapeMI](images/bastianl_photo.JPG)

**Short bio**
Lennart is a PhD candidate at TU Munich's CAMP lab under Prof. Nassir Navab, and an incoming research fellow at Imperial College London. Originally trained in applied mathematics (with early stints in NYC and California's tech scene), he found his calling at the intersection of geometry, machine learning, and clinical applications. His work focuses on making sense of the real world in 3D, teaching computers to understand complex surgical environments through a geometric lens.
His research has appeared at venues like MICCAI, IPCAI, CVPR, ICCV, and NeurIPS, with recent highlights including an oral presentation at ICCV '25 on predicting rotational dynamics and best paper award at the MICCAI â€˜25 COLAS workshop. He regularly reviews at top machine learning and computer vision conferences, and was recently recognized as an outstanding reviewer at CVPR '25. Beyond his own research, Lennart coordinates CAMP's Surgical Data Science team and has led industry collaborations bringing innovations in computer vision closer to clinical practice, bridging the gap between theory and real-world medical applications with the goal of improving patient care.

**Talk abstract**
3D Shape analysis has emerged as an important tool in medical imaging and computer-assisted interventions, from modeling population anatomy to registration and tracking of organs and tools. Abstracting useful information from 3D shapes often requires sophisticated computational tools based on manifold optimization and discrete differential geometry. These allow us to understand and perform precise operations on shapes, for example, putting them in correspondence with a reference. In this talk, I will address fundamentals on both these fronts, beginning with optimization formulations that enable many matching applications in computer vision. Of particular interest is how these formulations change when adapted to the manifold nature of 3D shapes and the ramifications for deep learning algorithms. I then address how geometric priors and our understanding of physical operators on the shape surface can be used to extract curvature signatures, and how these can be leveraged for correspondence problems. Finally, we will discuss limitations and open problems, and where these tools can best be employed.
